{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "NLP Workshop - UC Data Day.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OpWINZOsdiCX",
        "3Nm6dMeKdiCX",
        "9xYCVLYZdiCd",
        "rJEqkNy6diCg",
        "QTSPW1JidiCj",
        "qr5i56U8diCx",
        "KyGUbouQdiC0",
        "QuEW4arbdiC4",
        "WpMnOrmxdiC5",
        "y35CFCUgdiC8",
        "sv01L4HsdiC-",
        "YzJTeV1ddiDL",
        "B_HvsZ1ydiDO",
        "61eSaBGVdiDR",
        "LkuCis6UdiDS",
        "EOApnTJEdiDV",
        "9M9DNSX2diDZ",
        "9svug5IxdiDe",
        "RkwsFUVfdiDi",
        "ApsWtjxsdiDk",
        "M4zaTj6idiDn",
        "k910szrxdiDn",
        "mNlNbwXUdiDq",
        "Rn9cjKwhdiDr",
        "2Z4SZp0AdiDu",
        "PkKxdaiddiDx",
        "rdnwNbaIdiD2",
        "5FSxuPMAdiD6",
        "JFKZU5vzdiED",
        "KsEU6n68diEF",
        "f-bneobgdiEI",
        "bv2FV-Y2diEL",
        "L0ijp7SmdiEP",
        "4aFd8kUgdiEW",
        "tekojPGJdiEd",
        "KwvuW_mFdiEg",
        "3spe6cZXdiEl"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfUvGMFPdiCD"
      },
      "source": [
        "# Machines & Language: An Introduction to Natural Language Processing [NLP]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPD2tbHcdiCE"
      },
      "source": [
        "Welcome! In this workshop, we are going to learn how to go through the process of performing some _Machine Learning_ tasks on a set of text data. To do so we will (1) **Download** a corpus of text data to work with (2) **Extract Features** from this data, (3) **Use an Algorithm to Train a Classifier** which will then classify previously unseen data into a set of predefined categories (_Supervised ML_), then (4) **Attempt a Topic Model** using LDA (_UNsupervised ML_).\n",
        "\n",
        "#### \"Machine learning is a research field that sits at the intersections of statistics, artificial intelligence, and computer science. It is also known as predictive analystics or statistical learning.\"\n",
        "-- Andreas Mueller, \"Introduction to Machine Learning with Python\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPm7K-JLdiCF"
      },
      "source": [
        "## Learning objectives\n",
        "\n",
        "In this part of the workshop, you will begin to understand the following skills:\n",
        "\n",
        "* Understanding the logic in broad-ish strokes behind some ML tasks\n",
        "* Getting your eye familiar with some **Python** syntax\n",
        "* Using techniques from the **Natural Language Toolkit (NLTK)** for a classification task\n",
        "* Building a text classification system that can predict whether sentences belong to one category or another\n",
        "* Using the **scikit-learn** package (in Python) to perform some ML task(s) on the data\n",
        "* Evaluating the results of machine learning algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqe6oloAdiCG"
      },
      "source": [
        "## Key terms\n",
        "\n",
        "**Machine Learning**: An application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed\n",
        "\n",
        "**Corpus**: A large collection of data. In our case, this will be text data (although a corpus can contain any type of data)\n",
        "\n",
        "**Dataset**: collection of related information (such as a corpus)\n",
        "\n",
        "**Variable**: attribute of the dataset (such as the type of text being analyzed)\n",
        "\n",
        "**Features**: Properties that describe data attributes for machine learning - often the variables\n",
        "\n",
        "**Feature representation, feature vector**: A set of features\n",
        "\n",
        "**Supervised Machine Learning**: A machine learning task of learning a function that maps an input to an output based on example input-output pairs\n",
        "\n",
        "**Unsupervised Machine Learning**: A machine learning task used to draw inferences from datasets consisting of input data without labeled responses (lacks input-output pairs; only has input data)\n",
        "\n",
        "**Algorithm**: A process or set of rules to be followed in calculations (or other problem-solving operations), particularly by a computer\n",
        "\n",
        "**Classification**: An machine learning task used to predict a class label, which is a choice from a predefined list of possibilities\n",
        "\n",
        "**Observation**: entry in the dataset (a single text)\n",
        "\n",
        "**Measurement**: single data point (eg: one text's type)\n",
        "\n",
        "*Sources: Wikipedia, Andreas Mueller's \"Introduction to Machine Learning with Python\"*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiLIESLZdiCH"
      },
      "source": [
        "### What do you need for this workshop?\n",
        "* ~~Python 3.* / Jupyter Notebook~~ > Colaboratory Link!\n",
        "\n",
        "### Python Packages Overview\n",
        "* The Natural Language Toolkit (nltk) - we'll be accessing corpora as well as functional tools from this package.\n",
        "    * The Brown Corpus: A text corpus of American English, split into fifteen different categories\n",
        "    * Part of Speech Taggers (pos): prebuilt functions that are designed to determine the part of speech of every word in a given sentence.\n",
        "* pandas - for data processing    \n",
        "* matplotlib - for visualizing data (%matplotlib inline - displays images clearly in the Jupyter notebook)\n",
        "* scikit-learn - for machine learning (ft. various classification, regression and clustering algorithms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3YM41AadiCG"
      },
      "source": [
        "## Importing Python Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jceEWLsdiCJ"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVME3jtidiCN"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk import pos_tag_sents\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyXNvlZIdiCP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "518356b2-f89e-4759-edf1-c96655fc7bb5"
      },
      "source": [
        "nltk.download ('brown')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIPfkkhbdiCS"
      },
      "source": [
        "## Understanding Classification\n",
        "\n",
        "### How would you describe apples to a computer? How do they differ from oranges?\n",
        "Remember, computers can only really understand numbers, true false values, and strings within a predefined set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyFszEI4diCT"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/ErinEMcC/Louisville_ML/master/images/fruit1.png \"fruit1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjM5nuMUdiCU"
      },
      "source": [
        "_Source: Andrew Rosenberg_\n",
        "\n",
        "Our fruit test shows us everything we need to do a classification machine learning test. For each item with a label (apple, orange, lemon), we use a series of values to try to capture machine-understandable information about the item. These values are a feature representation of the item in question. The features themselves, as we can see above, can be numeric, true/false values, or a string in a set of predefined strings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U2xvSdLdiCU"
      },
      "source": [
        "### What if we had a new, unknown fruit?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDqUUT3YdiCV"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/ErinEMcC/Louisville_ML/master/images/fruit2.png \"fruit2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKd4COj-diCW"
      },
      "source": [
        "_Source: Andrew Rosenberg_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PqdBptndiCW"
      },
      "source": [
        "This fruit test is an example of a classification task. Classification allows you to predict a categorical value. This is a type of supervised machine learning, meaning we know the labels ahead of time and can give them to the machine learning algorithm so that it can be trained to knows what the categories of our data are. This way, when it comes time to give the  algorithm previously unseen data, it knows which categories it's looking for."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpWINZOsdiCX"
      },
      "source": [
        "## Let's get to coding!\n",
        "\n",
        "In this workshop we are going to classify two different sets of sentences from very different source material in the Brown corpus; one set of sentences from a corpus of news text, and the other set of sentences from a corpus of romance novel text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Nm6dMeKdiCX"
      },
      "source": [
        "## Accessing our Example Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWC1MteDdiCY"
      },
      "source": [
        "from nltk.corpus import brown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjCBPA_KdiCb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        },
        "outputId": "2aff56fe-91f4-44bc-cdf9-68c6ca8adf28"
      },
      "source": [
        "for category in brown.categories():\n",
        "    print (category)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5bd80f2ffe80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xYCVLYZdiCd"
      },
      "source": [
        "### Get the sentences from each corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cTQ6knOdiCe"
      },
      "source": [
        "news_sent = brown.sents(categories=[\"news\"])\n",
        "romance_sent = brown.sents(categories=[\"romance\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJEqkNy6diCg"
      },
      "source": [
        "### Let's Look at the first 5 sentences of each corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_XN6FDgdiCh"
      },
      "source": [
        "print (news_sent[:5])\n",
        "print ()\n",
        "print (romance_sent[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTSPW1JidiCj"
      },
      "source": [
        "## Organization of Data\n",
        "\n",
        "What do you notice about the format of the data above?\n",
        "\n",
        "Each sentence is already _tokenized_ split into a series of word and punctuation strings, with whitespace removed. This saves a lot of time having to do this work ourselves, manually. \n",
        "\n",
        "To start to organize our data, let's put these sentences into a pandas DataFrame, an object which has a format very similar to an Excel spreadsheet. We will first make two spreadsheets (one for news, and one for romance), and then combine them into one. We will also add the category each sentences came from, which will be our labels for each sentence and its associated feature representation (which we will build ourselves)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGn_U1wOdiCk"
      },
      "source": [
        "ndf = pd.DataFrame({'sentence': news_sent,\n",
        "                    'label':'news'})\n",
        "rdf = pd.DataFrame({'sentence':romance_sent, \n",
        "                    'label':'romance'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEuuHG8XdiCo"
      },
      "source": [
        "# combining two spreadsheets into 1\n",
        "df = pd.concat([ndf, rdf])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJuooKQVdiCq"
      },
      "source": [
        "Let's see what this DataFrame looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z7Fk2EHdiCr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "269e1a30-6832-43c2-b978-19aabb9f4b89"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[The, Fulton, County, Grand, Jury, said, Frida...</td>\n",
              "      <td>news</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[The, jury, further, said, in, term-end, prese...</td>\n",
              "      <td>news</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[The, September-October, term, jury, had, been...</td>\n",
              "      <td>news</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[``, Only, a, relative, handful, of, such, rep...</td>\n",
              "      <td>news</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[The, jury, said, it, did, find, that, many, o...</td>\n",
              "      <td>news</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4426</th>\n",
              "      <td>[Nobody, else, showed, pleasure, .]</td>\n",
              "      <td>romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4427</th>\n",
              "      <td>[Spike-haired, ,, burly, ,, red-faced, ,, deck...</td>\n",
              "      <td>romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4428</th>\n",
              "      <td>[``, Hello, ,, boss, '', ,, he, said, ,, and, ...</td>\n",
              "      <td>romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4429</th>\n",
              "      <td>[``, I, suppose, I, can, never, expect, to, ca...</td>\n",
              "      <td>romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4430</th>\n",
              "      <td>[``, I'm, afraid, not, '', .]</td>\n",
              "      <td>romance</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9054 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence    label\n",
              "0     [The, Fulton, County, Grand, Jury, said, Frida...     news\n",
              "1     [The, jury, further, said, in, term-end, prese...     news\n",
              "2     [The, September-October, term, jury, had, been...     news\n",
              "3     [``, Only, a, relative, handful, of, such, rep...     news\n",
              "4     [The, jury, said, it, did, find, that, many, o...     news\n",
              "...                                                 ...      ...\n",
              "4426                [Nobody, else, showed, pleasure, .]  romance\n",
              "4427  [Spike-haired, ,, burly, ,, red-faced, ,, deck...  romance\n",
              "4428  [``, Hello, ,, boss, '', ,, he, said, ,, and, ...  romance\n",
              "4429  [``, I, suppose, I, can, never, expect, to, ca...  romance\n",
              "4430                      [``, I'm, afraid, not, '', .]  romance\n",
              "\n",
              "[9054 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QWNslhSdiCu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "afd3e8d0-bc89-4f5c-f686-0afdc81bc9ce"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[The, Fulton, County, Grand, Jury, said, Frida...</td>\n",
              "      <td>news</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[The, jury, further, said, in, term-end, prese...</td>\n",
              "      <td>news</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[The, September-October, term, jury, had, been...</td>\n",
              "      <td>news</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[``, Only, a, relative, handful, of, such, rep...</td>\n",
              "      <td>news</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[The, jury, said, it, did, find, that, many, o...</td>\n",
              "      <td>news</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence label\n",
              "0  [The, Fulton, County, Grand, Jury, said, Frida...  news\n",
              "1  [The, jury, further, said, in, term-end, prese...  news\n",
              "2  [The, September-October, term, jury, had, been...  news\n",
              "3  [``, Only, a, relative, handful, of, such, rep...  news\n",
              "4  [The, jury, said, it, did, find, that, many, o...  news"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr5i56U8diCx"
      },
      "source": [
        "### So how many texts are there of each type?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf8bJLEAdiCx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "63be6ecb-d524-495a-f5a9-d3ce663a467b"
      },
      "source": [
        "df['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "news       4623\n",
              "romance    4431\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyGUbouQdiC0"
      },
      "source": [
        "### What if we want to visualize that information?\n",
        "\n",
        "We first create a figure and axes on which to draw our charts using plt.subplots(). Each chart is one axes, and a figure can contain multiple charts. Our data is encapsulated in df['label'].value_counts(), which is itself a dataframe. We then tell the Pandas to visualize the dataframe as a bar chart using .plot.bar(ax=ax, rot=0). The ax keyword tells Pandas which chart in the figure to plot, and the rot keyword controls the rotation of the x axis labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey6br_XbdiC0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "699c4ce7-6e65-488c-fa69-b5b77c7ed4b4"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "_ = df['label'].value_counts().plot.bar(ax=ax, rot=0)\n",
        "fig.savefig(\"categories_counts.png\", bbox_inches = 'tight', pad_inches = 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCqMDvwOdiC3"
      },
      "source": [
        "We have slightly more news texts that romance texts, which we should keep in mind as we go ahead with classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuEW4arbdiC4"
      },
      "source": [
        "## Extracting Features\n",
        "\n",
        "### Defining Features\n",
        "What should we use as features for the datset? What did we use for the fruit example before?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YutNO8-diC4"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/ErinEMcC/Louisville_ML/master/images/fruit1.png \"fruit1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olQMILyydiC5"
      },
      "source": [
        "Now that we are using sentences, how can we best represent each sentence as a series of values?\n",
        "One idea is to use how many particular parts of speech the sentence contains.\n",
        "\n",
        "* Nouns: Most basically described as a person, place, or thing. Counting nouns can help determine how many topics are being discussed in a sentence.\n",
        "* Adjectives: Descriptors of nouns (eg. \"yellow\", \"angry\", \"charming\"). Counting adjectives can help determine how often descriptive words are being added to nouns, which can demonstrate writing style."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpMnOrmxdiC5"
      },
      "source": [
        "### Parts of Speech\n",
        "\n",
        "Let us first take a look at all of the parts of speech (POS) on each sentence in our dataframe. The sentences are located in the column sentence, and to get the parts of speech, we can use the function pos_tag_sents from the NLTK package. Let's do that and look at the first five results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G29bM6gndiC5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "96aca92c-e18f-43d8-8ed2-8bcabcade686"
      },
      "source": [
        "# compute parts of speech on each sentence (row)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos_all = pos_tag_sents(df['sentence'])\n",
        "print (pos_all[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-33492a8dba5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# compute parts of speech on each sentence (row)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'averaged_perceptron_tagger'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpos_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpos_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y35CFCUgdiC8"
      },
      "source": [
        "#### What's with those part of speech labels? They aren't helpful at all!\n",
        "\n",
        "The Penn Tagset, which NLTK uses for it's part of speech tagger, is not particularly intuitive. Fortunately, they provide code that allows you to check what different tags stand for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRKSiIGjdiC8"
      },
      "source": [
        "# troubleshooting: https://github.com/nltk/nltk/issues/919\n",
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset(\"NN\")\n",
        "nltk.help.upenn_tagset(\"JJ\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv01L4HsdiC-"
      },
      "source": [
        "## Calculating our Features\n",
        "#### We can run a function that calculates our features across the dataset for us. In this case, numbers of nouns, adjectives, and adverbs that appear in the sentence\n",
        "\n",
        "Now we know the tags for the different parts of speech we want to count in each sentence. Let's now write a function that will count the parts of speech to us, when given a part of speech tagged sentence (such as we have already in our DataFrame) and the part of speech we want to count (for example, \"NN\" to count the number of nouns in the sentence)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zwor3OAxdiC_"
      },
      "source": [
        "def countPOS(pos_tag_sent, POS):\n",
        "    pos_count = 0\n",
        "    all_pos_counts = []\n",
        "    for sentence in pos_tag_sent:\n",
        "        for word in sentence:\n",
        "            tag = word[1]\n",
        "            if tag [:2] == POS:  \n",
        "                pos_count = pos_count+1\n",
        "        all_pos_counts.append(pos_count)\n",
        "        pos_count = 0\n",
        "    return all_pos_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PthDz4CwdiDB"
      },
      "source": [
        "We will now call this function twice, one for each of the parts of speech we are counting. As we finish counting them, we put the results into the DataFrame, saving us the trouble of having to do so late"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIbqsYgpdiDB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "3977a478-e718-4b19-bede-17618c84871d"
      },
      "source": [
        "df['NN'] = countPOS(pos_all, 'NN')\n",
        "df['JJ'] = countPOS(pos_all, \"JJ\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-9b80ce1eb906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NN'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcountPOS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'JJ'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcountPOS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"JJ\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pos_all' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEnBM8LAdiDE"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQXHZt33diDG"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzJTeV1ddiDL"
      },
      "source": [
        "### So how many POS types do we have for each type of text?\n",
        "\n",
        "We can use the Pandas groupby function to aggregate our data based on unique values in any column of the data. Here we seperate our data into groups by the label type (news or romance) and then add together each texts count of nouns, adjectives, and adverbs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4w0oBFediDL"
      },
      "source": [
        "df.groupby('label').sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_HvsZ1ydiDO"
      },
      "source": [
        "### Save the dataframe to your computer as a csv file (comma separated value)\n",
        "\n",
        "Pandas provides an easy function to save your DataFrames to your computer as a .csv file, a text file containing all the information separated by commas. The function is called to_csv:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3ZDQi_ydiDO"
      },
      "source": [
        "df.to_csv(\"df_news_romance.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61eSaBGVdiDR"
      },
      "source": [
        "## Supervised Machine Learning\n",
        "\n",
        "Supervised machine learning takes places in two steps: the training phase, and the testing phase. In the training phase, you use a portion of your data to train your algorithm (which, in our case, is a classification algorithm). You provide both your feature vector and your labels to the algorithm, and the algorithm searches for patterns in your data that can help associate it with a particular label.\n",
        "\n",
        "In the testing phase, we use the classifier we trained in the previous step, and give it previously unseen feature vectors representing unseen data to the algorithm, and have the algorithm predict the label. We can then compare the \"true\" label to the predicted label, and see if our classifier provides us with a good and generalizable way of accomplishing the task (in our case, the task of automatically distinguishing news sentences from romance sentences)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx98F2ocdiDR"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/ErinEMcC/Louisville_ML/master/images/mlsteps.png \"mlsteps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3GvaCs_diDR"
      },
      "source": [
        "_Source: Andrew Rosenberg_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkuCis6UdiDS"
      },
      "source": [
        "## QUIZ TIME! What are our Feature Vectors for the Brown Corpus example?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmnnwVB7diDS"
      },
      "source": [
        "It's important to remember that we cannot use the same data we used to build the classifier to test the data; if we did, our classifier would be 100% correct all of the time! This will not tell us how our trained classifer will perform on new, unseen data. We therefore need to split our data into a train set and a test set.\n",
        "\n",
        "* We will use the train set data to train our classifier\n",
        "* We will use the test set data to test our classifier\n",
        "\n",
        "First, we need to load in the Python libraries that we will be using for our analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VGDnrz2diDT"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk import pos_tag_sents\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOApnTJEdiDV"
      },
      "source": [
        "### Read data in from a spreadsheet\n",
        "Let's take the data we just saved out and load it back into a dataframe so that we can do some analysis with it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kmlmk0QOdiDW"
      },
      "source": [
        "df = pd.read_csv(\"df_news_romance.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M9DNSX2diDZ"
      },
      "source": [
        "### Preparing data for machine learning\n",
        "We're almost ready to do some machine learning! First, we need to split our data into feature vectors and labels. We need them separated to train the classifier. Remember, the features we are using to train our classifier are numbers of nouns, and adjectives in each sentence. (We are not using the sentences themselves as features!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvzOEVtKdiDZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fc234d0a-a311-412b-d1bc-e164b72dbde9"
      },
      "source": [
        "fv = df[[\"NN\", \"JJ\"]]\n",
        "fv.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NN</th>\n",
              "      <th>JJ</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   NN  JJ\n",
              "0  11   2\n",
              "1  13   2\n",
              "2  16   2\n",
              "3   9   3\n",
              "4   5   3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63LvzwJwdiDe"
      },
      "source": [
        "We have more news sentences than romance sentences; this is not a problem, but it's something to take note of during evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejmgAJKXdiDb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "40420f2f-93ae-4d5d-b37e-6c39712246e2"
      },
      "source": [
        "df['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "news       4623\n",
              "romance    4431\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9svug5IxdiDe"
      },
      "source": [
        "### Partitioning data into train and test sets\n",
        "When you are partitioning your data into train and test sets, a good place to start is to use 75% of your data for training, and 25% of your data for testing. We want as much training data as possible, while also having enough testing data to ensure that our trained classifier is generalizable across a number of examples. This will also lead to more accurate evalutation of our trained classifier.\n",
        "\n",
        "Fortunately, sklearn has a function that will do exactly this!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDT1NEyVdiDf"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(fv, df['label'],\n",
        "                                                stratify=df['label'], \n",
        "                                                test_size=0.25,\n",
        "                                                    random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkwsFUVfdiDi"
      },
      "source": [
        "* We use the \"stratify\" argument because we have an uneven amount of training data; we have more news sentances than romance sentences. By using stratify, we ensure that our classifier will take this data imbalence into account.\n",
        "\n",
        "* In this example, we are using a fixed random state, to ensure we will always get exactly the same value when we classify. Adding this argument is unnecessary for most types of classification; we do it here to ensure our results do not vary slightly across runs.\n",
        "\n",
        "## Fun Fact Alert!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HZO0NbodiDi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "521e17e3-e89c-4102-d222-5b2f38497b1d"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6790, 2)\n",
            "(2264, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApsWtjxsdiDk"
      },
      "source": [
        "### What classifier do I use?\n",
        "\n",
        "Choosing a classifier can be a challenging task. However, this flowchart can give you an idea of where to start!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O3rDQmSdiDl"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/ErinEMcC/Louisville_ML/master/images/algorithms_cheatsheet.png \"algorithms_cheatsheet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzIN6yFUdiDm"
      },
      "source": [
        "_Source: Andreas Mueller_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4zaTj6idiDn"
      },
      "source": [
        "## QUIZ TIME! Given what we know about our data and our goals, following the chart above & decide which algorithm we should use. \n",
        "\n",
        "Answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k910szrxdiDn"
      },
      "source": [
        "#### An animated example of classification\n",
        "The following animated GIF shows an example of linear classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSQjVRc2diDo"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/ErinEMcC/Louisville_ML/master/images/croppedml.gif \"cropped_ml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW89lxUZdiDp"
      },
      "source": [
        "_Source: Andrew Rosenberg_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNlNbwXUdiDq"
      },
      "source": [
        "## Supervised Classification Algorithm with _sklearn_\n",
        "\n",
        "One of the best things about sklearn is the simplicity of its syntax.\n",
        "\n",
        "To do machine learning with sklearn, follow these three steps (the function names remain the same, regardless of the classifier you use!):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn9cjKwhdiDr"
      },
      "source": [
        "### Step 1: Import your desired classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LmhXa8fdiDr"
      },
      "source": [
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z4SZp0AdiDu"
      },
      "source": [
        "### Step 2: Create an Instance of your machine learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DS2xntHdiDv"
      },
      "source": [
        "classifier = LinearSVC(max_iter=5000, dual=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkKxdaiddiDx"
      },
      "source": [
        "### Step 3: Fit your data to your classifier (TRAIN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEqBgtmtdiDy"
      },
      "source": [
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH-c3pxFdiD0"
      },
      "source": [
        "As mentioned above, LinearSVC, is a linear model for classification that separates classes using a line, a plane, or a hyperplane. The classifier.fit method searches for that line, plane, or hyperplane-which is also called the decision boundary. The dark gray line in the figure below is the decision boundary that the LinearSVC classifier found for this set of training data. All the data (dots) to the left of the gray line in the area with the orange background are classified as romance, while all the data to the right in the blue area are classified as news. The leftward skew of the classification space is due to the data being very dense and highly overlapping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDipCloqdiD1"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/ErinEMcC/Louisville_ML/master/images/training_boundary.png \"training_boundary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdnwNbaIdiD2"
      },
      "source": [
        "### Step 4: Predict Labels for Unseen Data (TEST)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3dpqNjMdiD3"
      },
      "source": [
        "y_predict = classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FSxuPMAdiD6"
      },
      "source": [
        "### Step 5: Score!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIcL3OJXdiD6"
      },
      "source": [
        "classifier.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIx_KkuXdiD-"
      },
      "source": [
        "Right now, our classifier can correctly predict previously unseen news and data about 71% of the time. We can get more information about how we are doing by creating a confusion matrix. This confusion matrix shows how many times we are predicting categories correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5VvHkcSdiD-"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A7gU7FQdiEA"
      },
      "source": [
        "confusion_matrix(y_test, y_predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TetlQ2tHdiEC"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/ErinEMcC/Louisville_ML/master/images/confusion_matrix.png \"confusion_matrix\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYCM_seudiED"
      },
      "source": [
        "In LinearSVC the classifier.predict decides which class a data point is in based on which side of the decision boundary, which is the gray line in the figure, the point falls on. Points in the orange area to the left of the gray line are classified as romance, while points in the blue area to the right of the gray line. Orange points in the blue area are romance texts that are misclassified as news texts, while blue points in the orange area are news texts that are misclassified as romance texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPsJ59xXdiED"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/ErinEMcC/Louisville_ML/master/images/testing_boundary.png \"testing_boundary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFKZU5vzdiED"
      },
      "source": [
        "## Unsupervised Machine Learning\n",
        "\n",
        "In supervised machine learning tasks, the data is assigned to some set of classes. For example, here we are given a dataset wherein each observation is a set of physical attributes of an object. In an supervised task, the object column acts as the labels. The algorithm then uses these existing separations in the data to develop criteria for classifying unknown observations in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUIkfHzGdiEE"
      },
      "source": [
        "In contrast, in an unsupervised machine learning task there either are no labels or that information is just treated as another attribute of the observation. In our fruit example, the object type is now just another characteristic of the observation, and often is altogether unknown:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VjMQ4HfdiEE"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/ErinEMcC/Louisville_ML/master/images/fruit1.png \"fruit1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV-xuVaOdiEE"
      },
      "source": [
        "An unsupervised algorithm is not told how the data is structured or separated (barring parameter tuning); instead the algorithm goes looking for stucture and separation in the data.\n",
        "\n",
        "Clustering algorithms aim to group the observations in the data into categories (classes) based on some notion of how similar the observations are to each other. For example, given a basket of fruit, a clustering algorithm tries to group what it thinks are apples together into one class, and what it thinks are oranges into another.\n",
        "\n",
        "Dimension reduction techniques aim to decrease the number of rows and columns in a dataset based on some criteria such as which variables most separate the observations. For example, given the height, width, color, mass, and roundness of the fruit attributes, one dimension reduction algorithm will try to determine the minimum number of attributes needed to tell the fruit apart—can we tell it's an apple with just the mass and color?\n",
        "\n",
        "Generally speaking, in an unsupervised task there is no existing labeling to compare the results of the algorithm to; instead we often evaluate reliability through repeated experiments, computing the odds of our data being generated by our model, and visualizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qjJ0lJjdiEF"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/ErinEMcC/Louisville_ML/master/images/algorithms_cheatsheet.png \"algorithms_cheatsheet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsEU6n68diEF"
      },
      "source": [
        "## Topic Modeling with Latent Dirchlet Allocation (LDA)\n",
        "\n",
        "One subset of unsupervised learning tasks are topic extraction tasks, where the aim is to find common groupings of items across collections of items. One method of doing so is Latent Dirichlet allocation (LDA). Latent Dirichlet Allocation is a way to model how topics are distributed over a corpus and words are distributed over a set of topics.\n",
        "\n",
        "In broad strokes, LDA extracts hidden (latent) topics via the following steps:1, 2\n",
        "\n",
        "1. Arbitrarily decide that there are 10 topics.\n",
        "2. Select one document and randomly assign each word in the document to one of the 10 topics.\n",
        "3. Repeat step 2 for all the other documents. This results in the same word being assigned to multiple topics.\n",
        "4. Compute\n",
        "    * how many topics are in each document?\n",
        "    * how many topic assignements are due to a given word?\n",
        "5. Take one word in one document and reassign it to a new topic and then repeat step 4.\n",
        "6. Repeat step 5 until the model stabilizes such that reassigned topics do not change distributions.\n",
        "\n",
        "LDA yields a set of words associated to each topic (see step 4, part 2) and the mixture of topics associated to each document (see step 4, part 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ_7AASRdiEF"
      },
      "source": [
        "# Let's do topic modeling with sklearn!\n",
        "\n",
        "One of the best things about sklearn is the simplicity of its syntax.\n",
        "\n",
        "To do machine learning with sklearn, follow these five steps (the function names remain the same, regardless of the algorithm you use!):\n",
        "\n",
        "### Step 1: Import your desired algorithm\n",
        "\n",
        "In this example, we will be using the Latent Dirichlet Allocation algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr4VIWnLdiEG"
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-bneobgdiEI"
      },
      "source": [
        "### Step 2: Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "When creating an instance of sklearn's _Latent Dirichlet Allocation_ algorithm to run on our data, we need to set parameters. n_components is the number of topics in the dataset and we set random_state to 42 so that this notebook is reproducible. Since the sentences happen to already have labels (either news or romance), lets see if LDA can also find those separations by setting the number of topics to 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_I6AccadiEI"
      },
      "source": [
        "num_topics = 2\n",
        "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv2FV-Y2diEL"
      },
      "source": [
        "### Step 3: Fit your data\n",
        "\n",
        "Using the lda object we set up above, we now apply (fit) the LDA algorithm to the bag of words we extracted from our sentences and had stored in the tf sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk3RgJC2diEL"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "tf_vectorizer = CountVectorizer(stop_words='english')\n",
        "tf = tf_vectorizer.fit_transform(df['sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8IOAmbfdiEN"
      },
      "source": [
        "lda.fit(tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0ijp7SmdiEP"
      },
      "source": [
        "### Step 4: Transform your data\n",
        "\n",
        "We now want to model the documents in our corpus in terms of the topics discovered by the model. This is done using the .transform method of LDA. This function yields the distribution of topics across the documents. The document_topic array contains the percentages of each topic found in each document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtPyP9K8diEQ"
      },
      "source": [
        "document_topic = lda.transform(tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIdi45etdiEU"
      },
      "source": [
        "Then we visualize how much of each document is each topic - for example that document 1 is 10% topic A and 25% topic b. We choose an area chart because each band of the chart maps to a different category (in this case a unique topic). The width of each band in relation to the others illustrates how much of the document is thought to be about that topic relative to the others."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQd0j7qtdiEU"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from cycler import cycler\n",
        "import numpy as np\n",
        "\n",
        "colors = ['tab:green', 'tab:pink']\n",
        "topics = np.arange(10)\n",
        "num_docs = document_topic.shape[0]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,5))\n",
        "_ = ax.stackplot(range(num_docs), document_topic.T, labels=topics, colors=colors)\n",
        "_ = ax.set_xlim(0, num_docs)\n",
        "_ = ax.set_ylim(0,1)\n",
        "_ = ax.set_yticks([])\n",
        "_ = ax.set_xlabel(\"document\")\n",
        "_ = ax.legend(title=\"topic\", bbox_to_anchor=(1.06, 1), borderaxespad=0)\n",
        "fig.savefig(\"images/doc_topic.png\", bbox_inches = 'tight', pad_inches = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aFd8kUgdiEW"
      },
      "source": [
        "### Step 5: Print topics\n",
        "\n",
        "lda.components_ is an array where each row is a topic, and each column roughly contains the number of times that word was assigned to that topic, which is also the probability of that word being in that topic. To figure out which word is in which column, we use the get_feature_names() function from CountVectorizer. The argsort function is used to return the indexes of the columns with the highest probabilities, which we then map into our collection of words. Here we print the top 5 words in each topic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAlLgqrzdiEW"
      },
      "source": [
        "num_words = 10\n",
        "topic_word  = lda.components_ \n",
        "words = np.array(tf_vectorizer.get_feature_names())\n",
        "for i, topic in enumerate(topic_word):\n",
        "    # sorting is in descending, so ::-1 reverses to ascending\n",
        "    sorted_idx = topic.argsort()[::-1]\n",
        "    print(i, words[sorted_idx][:num_words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obytTDswdiEa"
      },
      "source": [
        "We can also visualize these topics as lists sized by the frequency of the word and colored by the topic, as proposed by _Allan Riddell_ in [Text Analysis with Topic Models for the Humanities and Social Sciences](https://liferay.de.dariah.eu/tatom/index.html):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRNs915fdiEb"
      },
      "source": [
        "# font size for word with largest share in corpus\n",
        "fontsize_base = 40/ np.max(topic_word)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 2), constrained_layout=True)\n",
        "\n",
        "for i, topic in enumerate(topic_word):\n",
        "    top_idx = topic.argsort()[::-1][:num_words]\n",
        "    top_words = words[top_idx]\n",
        "    top_share = topic[top_idx]\n",
        "    for j, (word, share) in enumerate(zip(top_words, top_share)):\n",
        "        ax.text(j, i/4,  word, fontsize=fontsize_base*share, color=colors[i])\n",
        "        \n",
        "#stretch the-axis to accommodate the words\n",
        "ax.set_xlim(0, num_words)\n",
        "ax.set_ylim(-.2, i/4+.2)\n",
        "ax.axis('off')\n",
        "#fig.subplots_adjust(hspace=-0)\n",
        "fig.savefig(\"images/word_topic.png\", bbox_inches = 'tight', pad_inches = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tekojPGJdiEd"
      },
      "source": [
        "### Step 6: Score!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBErWsmcdiEd"
      },
      "source": [
        "One method of evaluating a model is to compute the chance (probability) of the data we observed showing up in a dataset generated by the model. First we start with the modeled probability density function, which is the theoretical distribution of all topics in our model. We then use the log likelihood and the perplexity functions to evaluate the average odds of our observations occuring in the modeled distribution of words and topics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vMFIF-rdiEe"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/ErinEMcC/Louisville_ML/master/images/xkcd_False.png \"xkcd_False\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9VpVLMmdiEe"
      },
      "source": [
        "print(f'Approximate Log Likelihood: {lda.score(tf)}')\n",
        "print(f'Perplexity: {lda.perplexity(tf)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwvuW_mFdiEg"
      },
      "source": [
        "### Step 7: Add Supervision: Compare Topics to Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpVfMCqRdiEg"
      },
      "source": [
        "We can compare the results of our topic modeling to the labels we already have for the data. First we need to assign a label to each document based on which topic is most prevalant, which we can do using the argmax function since it returns the index (which maps directly to the topic) of the cell with the highest value. We then compare these topic based classes to the labels in our dataset. Given the sentences for each topic, we will make the assumption that topic 0 is news and topic 1 is romance. We canargmax returns the index of the"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my8Ob_sxdiEh"
      },
      "source": [
        "# get the location of the highest value in each column\n",
        "topic_class = document_topic.argmax(axis=1)\n",
        "topic_labels = np.empty(topic_class.shape, dtype=object)\n",
        "topic_labels[topic_class==0] = 'news'\n",
        "topic_labels[topic_class==1] = 'romance'\n",
        "topic_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2vtmR99diEj"
      },
      "source": [
        "We can now use a confusion matrix to see if there is overlap between the topics and the labels. In a confusion matrix, the data is the counts of true positive, false positive, false negative, and true negative labeing. The confusion matrix can be visualized as a table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRLHEpT6diEj"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(df['label'], topic_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3spe6cZXdiEl"
      },
      "source": [
        "#### Unfortunately\n",
        "LDA doesn't seem to work all that well for this dataset. And nothing about the topics indicates a distinction between the romance and news texts... but we already saw that they didn't seem to be all that separable. Can we get better results by expanding the corpus to include more texts of other types? Or by expanding each document so that it is longer than a sentence?\n",
        "\n",
        "Since topic modeling works better with longer texts, what topics might you get if you try to model: Moby Dick, Pride & Prejudice, Both together, some contemporary title?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeVU23WPdiEl"
      },
      "source": [
        "# Review\n",
        "\n",
        "At the end of this workshop, we have covered the following skills:\n",
        "* How to use skills from the NLTK workshop to build features for a classification task\n",
        "* How to build a text classification system that can predict whether sentences belong to one category (\"news\") or another (\"romance\")\n",
        "* How to group data and perform calculations on the aggregations\n",
        "* How to prepare data for machine learning using pandas, a package for Python that helps to organize your data\n",
        "* How to use the scikit-learn package for Python to perform different types of machine learning on the data\n",
        "* How to evaluate the results of machine learning algorithms\n",
        "* How to visualize observations, aggregations, and algorithmic results\n",
        "\n",
        "## Resources\n",
        "\"Introduction to Machine Learning with Python\", Andreas C. Muller and Sarah Guido. O'Reilly, 2017.\n",
        "\n",
        "\"LING 83800: Methods in Computational Linguistics II\", Andrew Rosenberg. http://eniac.cs.qc.cuny.edu/andrew/methods2/, 2014.\n",
        "\n",
        "\"Introduction to Latent Dirichlet Allocation\", Edward Chen, http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/, 08/22/2011\n",
        "\n",
        "\"Topic Modeling for Humanists: A Guided Tour\", Scott Weingart, http://www.scottbot.net/HIAL/index.html@p=19113.html, 07/25/2012\n",
        "\n",
        "\"The LDA Buffet is Now Open\", Matthew Jockers, http://www.matthewjockers.net/2011/09/29/the-lda-buffet-is-now-open-or-latent-dirichlet-allocation-for-english-majors/, 09/29/2011\n",
        "\n",
        "\"Introduction to Topic Modeling\",Christine Doig, http://chdoig.github.io/pytexas2015-topic-modeling/#/, PyTexas, 2015\n",
        "\n",
        "##### Acknowledgments\n",
        "Shout out to the CUNY DHRI for providing much of this workshop's structure.\n"
      ]
    }
  ]
}