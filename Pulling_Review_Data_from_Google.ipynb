{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pulling Review Data from Google.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hTUtYWVm6jto",
        "W71MJXb08NBR"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjWbZou6qoWF"
      },
      "source": [
        "#Collecting Review Data from Google-Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNw86pd4hRkM"
      },
      "source": [
        "The following is a documentation of the process of getting review data from Google Maps using a combination of the Google Places API and APIFY, a web scraping service (https://apify.com/). We use the Google Places API to get a list of businesses, and then we use APIFY to scrape the reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioTpkWiFMgXQ"
      },
      "source": [
        "Imports the libraries necessary for the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31ZJ4V1RZCw1"
      },
      "source": [
        "#Importing libraries\n",
        "import pandas as pd\n",
        "import requests, json\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZydgBgzNhq2e"
      },
      "source": [
        "You will need an API key for the Google Places API. This can be obtained by making a Google Cloud account. You can learn how to obtain and API key at this link: https://developers.google.com/maps/documentation/places/web-service/get-api-key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i82DDPRM14T"
      },
      "source": [
        "This creates a placeholder for the api_key and designates the URL that we will be using to attach parameters to. You would place your API key where it says 'api key goes here'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvykKFVnhoiu"
      },
      "source": [
        "api_key = 'api key goes here'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSoAKppbik77"
      },
      "source": [
        "#Our base URL that we will attach parameters to\n",
        "url = \"https://maps.googleapis.com/maps/api/place/textsearch/json?\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95sD5eHei2Qz"
      },
      "source": [
        "## If getting data from just one location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWCythovN-BL"
      },
      "source": [
        "This allows the user to input what they want to search for. The desired query is put into the variable labeled 'query'. The query they input, along with the api key following it, is then attached to the base url. This result is then held in the variable r. r is then converted to a json object. The json(s) will pull the desired information from the original query and store it in a dataframe. The dataframe contains the attributes: bizName, placeID, address, placeTypes, and numOfRatings. \n",
        "\n",
        "Each json object that is returned from the request will have at least 50 businesses. If there are more than 50 businesses, however, a next page needs generated. To do this, a while loop is utilized. The loop checks at least once to see if there are more businesses. The loop will continue to load businesses by checking to see if there is a next page until it returns that there isn't a next page. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSL25KQTjwWp"
      },
      "source": [
        "# This is the text string on which to search. This should be altered depending on what you are looking for. \n",
        "# For example, you might set query as \"Honky Tonk in Nashville, TN\" or \"Coffee shop in San Francisco\".\n",
        "\n",
        "query = \"Urgent Care in Cincinnati\"\n",
        "  \n",
        "#The query and API key are attached to the base URL\n",
        "r = requests.get(url + 'query=' + query +\n",
        "                        '&key=' + api_key)\n",
        "  \n",
        "#we convert the response from requestions into a json object\n",
        "currentJSON = r.json()\n",
        "\n",
        "\n",
        "#This is the dataframe where we'll keep all the info we get from the json(s). This initializes an empty dataframe with some defined columns.\n",
        "holdingDF = pd.DataFrame(columns=[\"bizName\", \"placeID\", \"address\", \"placeTypes\", \"numOfRatings\"])\n",
        "\n",
        "#each json we get from our get request will have 50 businesses. If there are more businesses, there will be a 'next_page_token'.\n",
        "#we want the following while loop to run at least once regardless of what we get in our json, so we set isToken to True.\n",
        "isToken = True\n",
        "\n",
        "while isToken:\n",
        "  for business in range(len(currentJSON[\"results\"])):\n",
        "    if 'user_ratings_total' in currentJSON[\"results\"][business]:\n",
        "      bizName = currentJSON[\"results\"][business]['name']\n",
        "      placeID = currentJSON[\"results\"][business]['place_id']\n",
        "      address = currentJSON[\"results\"][business]['formatted_address']\n",
        "\n",
        "      allTypes = \"\"\n",
        "      for eachType in currentJSON[\"results\"][business]['types']:\n",
        "        allTypes = allTypes + \", \" + eachType\n",
        "      allTypes = allTypes[1:]\n",
        "      \n",
        "      numOfRatings = currentJSON[\"results\"][business]['user_ratings_total']\n",
        "\n",
        "      new_row = {'bizName': bizName,\n",
        "                'placeID': placeID,\n",
        "                'address': address,\n",
        "                'placeTypes': allTypes,\n",
        "                'numOfRatings': numOfRatings}\n",
        "      holdingDF = holdingDF.append(new_row, ignore_index=True)\n",
        "\n",
        "  time.sleep(1.5)\n",
        "  #The while loop checks if there is a next_page_token after the loop is complete, but we need this here to check if the FIRST page has one\n",
        "  #If it does, go ahead and load the next JSON. Do display(currentJSON) if you want to get a better idea of how this works.\n",
        "  if 'next_page_token' in currentJSON:\n",
        "    r = requests.get(url + '&key=' + api_key + '&pagetoken=' + currentJSON['next_page_token'])\n",
        "    \n",
        "  currentJSON = r.json()\n",
        "  if 'next_page_token' in currentJSON:\n",
        "    isToken = True\n",
        "  else:\n",
        "    isToken = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbnK82_xY9Fb"
      },
      "source": [
        "This displays the results of the search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVX7kj1AnW0K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "c8b04f99-2947-4729-daff-111472ed72c4"
      },
      "source": [
        "display(holdingDF)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bizName</th>\n",
              "      <th>placeID</th>\n",
              "      <th>address</th>\n",
              "      <th>placeTypes</th>\n",
              "      <th>numOfRatings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [bizName, placeID, address, placeTypes, numOfRatings]\n",
              "Index: []"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfD0MdD-i5-c"
      },
      "source": [
        "## If getting data from many locations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcxFsaAFpPGJ"
      },
      "source": [
        "Maybe we don't want to perform just one search, maybe we want to perform several searches to get results from many different places. For example, what if we wanted to do search in every city in Ohio? This code can help us do that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1ByGNqppuDW"
      },
      "source": [
        "For this code, I used a CSV file containting every city in Ohio ordered by population. I got this list from this link: https://www.ohio-demographics.com/cities_by_population (it required a bit of cleaning)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1DLlLbgXu7v"
      },
      "source": [
        "This imports your CSV file and reads the contents of it into a variable. It will first have to mount your drive and then read in the .CSV from the designated place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "ENvU86CDpK1n",
        "outputId": "186a8a34-00c6-49c3-a964-91b7a8f9faca"
      },
      "source": [
        "#Importing location data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "cityDF = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/ERURproject/Yelp/OhioPopulationByCity.csv\", index_col= 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-fbae804f899c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcityDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Colab Notebooks/ERURproject/Yelp/OhioPopulationByCity.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/ERURproject/Yelp/OhioPopulationByCity.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snokVnmlX_2r"
      },
      "source": [
        "This first creates a blank dataframe to hold the results of the search using the .CSV file. This code is limited to the a search of ten different values, but the range can be changed within the code. A query is then generated using, in this case the city name, each of the desired searches. A request is then generated by combining the url, query, and api key. This request is then turned into a json object, so we can get our search results. For each json, the results are input as a new row in the dataframe. A while loop is utilized once again to determine if any next pages are needed, and they businesses will continue to be added until a next page doesn't need to be utilized anymore. This is completed for the ten search conditions specified.After that is complete, the duplicates are removed from the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il1EHz9fqX5X"
      },
      "source": [
        "holdingDF = pd.DataFrame(columns=[\"bizName\", \"placeID\", \"address\", \"placeTypes\", \"numOfRatings\"])\n",
        "counter = 0\n",
        "\n",
        "#I limited this search to the first 10 most populated cities in Ohio, hence the range(10).\n",
        "for city in range(10):\n",
        "\n",
        "  #The city name gets passed into the query based on the iteration index\n",
        "  query = \"Urgent Care in {}, Ohio\".format(cityDF.loc[city, 'City'])\n",
        "\n",
        "  r = requests.get(url + 'query=' + query +\n",
        "                          '&key=' + api_key)\n",
        "  currentJSON = r.json()\n",
        "  isToken = True\n",
        "  while isToken:\n",
        "    for business in range(len(currentJSON[\"results\"])):\n",
        "      if 'user_ratings_total' in currentJSON[\"results\"][business]:\n",
        "        bizName = currentJSON[\"results\"][business]['name']\n",
        "        placeID = currentJSON[\"results\"][business]['place_id']\n",
        "        address = currentJSON[\"results\"][business]['formatted_address']\n",
        "\n",
        "        allTypes = \"\"\n",
        "        for eachType in currentJSON[\"results\"][business]['types']:\n",
        "          allTypes = allTypes + \", \" + eachType\n",
        "        allTypes = allTypes[1:]\n",
        "        \n",
        "        numOfRatings = currentJSON[\"results\"][business]['user_ratings_total']\n",
        "\n",
        "        new_row = {'bizName': bizName,\n",
        "                  'placeID': placeID,\n",
        "                  'address': address,\n",
        "                  'placeTypes': allTypes,\n",
        "                  'numOfRatings': numOfRatings}\n",
        "        holdingDF = holdingDF.append(new_row, ignore_index=True)\n",
        "\n",
        "    \n",
        "    time.sleep(1.5)\n",
        "    if 'next_page_token' in currentJSON:\n",
        "      r = requests.get(url + '&key=' + api_key + '&pagetoken=' + currentJSON['next_page_token'])\n",
        "      \n",
        "    # json method of response object convert\n",
        "    #  json format data into python format data\n",
        "    currentJSON = r.json()\n",
        "    if 'next_page_token' in currentJSON:\n",
        "      isToken = True\n",
        "    else:\n",
        "      isToken = False\n",
        "  counter += 1\n",
        "  print(str(round((counter/10)*100, 1)) + \" percent complete\")\n",
        "\n",
        "#Since we're iterating over many cities, some of our search results may overlap. It's important to remove duplicates.\n",
        "holdingDF = pd.DataFrame.drop_duplicates(holdingDF)\n",
        "holdingDF.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M-AYpq_Z9p8"
      },
      "source": [
        "This displays the generated dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzBFT-xQtmfW"
      },
      "source": [
        "display(holdingDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTUtYWVm6jto"
      },
      "source": [
        "## Now that we have a dataframe full of businesses from the Google Places API, the next step is to create a list of URLs we can feed to APIFY."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At7QJr9KZOKs"
      },
      "source": [
        "A empty dataframe is created to hold the URLs with the attributes: name, validURL, placeID, address, city, state, and dataSource."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrXLSkRX7fiJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "c0cbee5e-250f-45c0-8281-637b264e7bf7"
      },
      "source": [
        "#Create a new empty dataframe\n",
        "businessDFurlsDF = pd.DataFrame(columns=[\"name\", \"validURL\", \"placeID\", \"address\", \"city\", \"state\", \"dataSource\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-90b9def5c02b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Create a new empty dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbusinessDFurlsDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"validURL\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"placeID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"address\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"city\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"state\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dataSource\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43vcwMr0ZauU"
      },
      "source": [
        "This code generates the list of URLs that we need to give to APIFY. First, a url is specified. Then using the dataframe we generated earlier to collect the information and the placeID of the business, we create a placeIDiter. This is then used to create a placeID request in a new request. The new request contains the url, placeID, name, rating, url, formatted_address, and api key. This request is then turned into a JSON object. From this object, we pull the business name. The placeID we are going to save is then located by the URL in the JSON response for the business. This URL will also contain a CID number that will be appended to the located URL. Adding this CID number allows APIFY to be able to read the URL. The address, city, and state are then calculated and populated. The name, validURL (that was just created), placeID, address, state, and dataSource are all added to a new row. This new row is then added to an existing dataframe. This is repeated for the placeID of every business. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqft2XEf7iVK"
      },
      "source": [
        "#For the placeID of every business...\n",
        "for placeIDindex in range(len(holdingDF)):\n",
        "  \n",
        "  url = \"https://maps.googleapis.com/maps/api/place/details/json?\"\n",
        "  placeIDiter = holdingDF.loc[placeIDindex, 'placeID']  \n",
        "\n",
        "  #Notice that we're now feeding requests a 'place_id=' parameter now\n",
        "  r = requests.get(url + 'place_id=' + placeIDiter +\n",
        "                          '&fields=' + \"name,rating,url,formatted_address\" + '&key=' + api_key)\n",
        "  currentJSON = r.json()\n",
        "\n",
        "  bizName = currentJSON['result']['name']\n",
        "\n",
        "  #This is the important part; the url in the json response for the business gotten by the placeID\n",
        "  #The url contains a 'CID number'. When we append this CID number to the URL form below\n",
        "  #It takes the form of a URL that APIFY can read. \n",
        "  placeIDtoSave = currentJSON['result']['url']\n",
        "  theGoldenCID = placeIDtoSave.split('cid=')[1]\n",
        "  validURL = 'https://www.google.com/maps/place/?cid=' + theGoldenCID\n",
        "\n",
        "  address = currentJSON['result']['formatted_address']\n",
        "\n",
        "  city = address.split(',')[1].strip()\n",
        "  state = address.split(',')[2].strip()[0:2]\n",
        "\n",
        "  #We've gotten some extra information like address ad city, but the important colimn is 'validURL'. \n",
        "  #This is what we'll use in the next step\n",
        "  new_row = {'name': bizName,\n",
        "            'validURL': validURL,\n",
        "            'placeID': placeIDiter,\n",
        "            'address': address,\n",
        "            'city': city,\n",
        "            'state': state,\n",
        "              'dataSource': \"GoogleMaps\"}\n",
        "  businessDFurlsDF = businessDFurlsDF.append(new_row, ignore_index=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A1QgASG8yOn"
      },
      "source": [
        "This code creates a new dataframe. Every validURL and its location in the previously generated dataframe containing all the validURLs, is then added to the new dataframe. Duplicates are then removed from the new dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIc6X6tk6geS"
      },
      "source": [
        "validPlaceIDlist = []\n",
        "\n",
        "for validURL in range(len(businessDFurlsDF)):\n",
        "  validPlaceIDlist.append(businessDFurlsDF.loc[validURL, 'validURL'])\n",
        "\n",
        "#remove duplicates\n",
        "newlist = list(set(validPlaceIDlist))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKkPfEtl9LDp"
      },
      "source": [
        "The new list generated is now displayed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MpfP_a88GHE"
      },
      "source": [
        "display(newlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W71MJXb08NBR"
      },
      "source": [
        "## Now download the list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go2Vh-5u9N02"
      },
      "source": [
        "This code downloads the new list that was just created. It imports the file from google.colab and turns it into a .csv file. The .csv file is then downloaded. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUsEWmrF8Qdw"
      },
      "source": [
        "from google.colab import files\n",
        "df = pd.DataFrame(newlist)\n",
        "df.to_csv('placeIDlistforAPIFY.csv', index=False)\n",
        "files.download('placeIDlistforAPIFY.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPA_O2R27-AW"
      },
      "source": [
        "## Now we go to APIFY. Create a new \"task\" and choose the Google Maps Scraper. Plug in the CSV list that we made in the last step to \"Start URLs\" (choose upload text file). Remove search terms. Set max crawled places to 0. Under output configuration set max reviews to 99999. Set max images to 0. Uncheck include popular times. Sort reviews by \"Newest\". Run the scraper. APIFY will give you a JSON file. Download it and bring it into this code. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct3ts5bJ-ELi"
      },
      "source": [
        "Using the previous code, I ran a search for \"Urgent Care in Cincinnati\", received 40 businesses, and put the CSV file in APIFY. I got back a JSON, which I'm importing into the code now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2snq1KV6ssZ"
      },
      "source": [
        "This code mounts your google drive and imports the JSON created above into dataFromAPIFYDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ4IudLm92zj"
      },
      "source": [
        "#Mount the drive in case we haven't already.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataFromAPIFYDF = pd.read_json(\"/content/drive/My Drive/Colab Notebooks/Google/GoogleTutorialData.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaWrzQX1-30-"
      },
      "source": [
        "## If you want to make a dataframe where each row is a separate review..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1coTw2sKlEVL"
      },
      "source": [
        "This code will output the dataframe with a view of each row being a separate review. First, a datafram is created to hold the reviews that has the attributes businessTitle, googleURL, CIDnum, phoneNum, address, city, state, businessRating, reviewDate, reviewText, reviewRating, bizType, and dataSource. Conditions can then be set to filter unwanted businesses from the APIFYDF dataframe where we will populate our results from. Here, we set conditions that will only select businesses that are an \"Urgent care center\" and are located in 'OH'. The same attributes located in the review dataframe are then pulled from the reviews in the APIFYDF dataframe and stored in variables. These attributes are then put into a new row, and the new row is added onto the review dataframe. This is repeated for every review until there are no more. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAkbaE7T_CPQ"
      },
      "source": [
        "#REVIEW SCALE\n",
        "\n",
        "\n",
        "reviewScaleDF = pd.DataFrame(columns=[\"businessTitle\", \"googleURL\", \"CIDnum\", \"phoneNum\", \"address\", \"city\", \"state\", \"businessRating\", \"reviewDate\", \"reviewText\", \"reviewRating\", \"bizType\", \"dataSource\"])\n",
        "counter = 0\n",
        "for business in range(len(dataFromAPIFYDF)):\n",
        "  #Our search will have given us businesses that are not urgent care centers, and since Cincinnati is very close to Kentucky, we may get some results from KY\n",
        "  #This next step filters results to only include businesses that are in Ohio and that have the categoryName 'Urgent care center'\n",
        "  if dataFromAPIFYDF.loc[business, 'categoryName'] == 'Urgent care center' and 'OH' in dataFromAPIFYDF.loc[business, 'address']:\n",
        "    for review in range(len(dataFromAPIFYDF.loc[business, 'reviews'])):\n",
        "\n",
        "      businessTitle = dataFromAPIFYDF.loc[business, 'title']\n",
        "      directURL = dataFromAPIFYDF.loc[business, 'placeId']\n",
        "      placeIDtoSave = dataFromAPIFYDF.loc[business, 'placeId']\n",
        "\n",
        "      theGoldenCID = placeIDtoSave.split('cid=')[1]\n",
        "\n",
        "      phoneNum = dataFromAPIFYDF.loc[business, 'phone']\n",
        "      address = dataFromAPIFYDF.loc[business, 'address']\n",
        "      city = address.split(',')[1].strip()\n",
        "      state = address.split(',')[2].strip()[0:2]\n",
        "\n",
        "      overallRating = dataFromAPIFYDF.loc[business, 'totalScore']\n",
        "      theDate = dataFromAPIFYDF.loc[business, 'reviews'][review]['publishedAtDate'][0:10]\n",
        "\n",
        "      reviewText = dataFromAPIFYDF.loc[business, 'reviews'][review]['text']\n",
        "      reviewRating = dataFromAPIFYDF.loc[business, 'reviews'][review]['stars']\n",
        "\n",
        "\n",
        "      if reviewText:\n",
        "        new_row = {'businessTitle': businessTitle,\n",
        "                  'googleURL': directURL,\n",
        "                  'CIDnum': theGoldenCID,\n",
        "                  'phoneNum': phoneNum,\n",
        "                  'address': address,\n",
        "                  'city': city,\n",
        "                  'state': state,\n",
        "                  'businessRating': overallRating,\n",
        "                  'reviewDate': theDate,\n",
        "                  \"reviewText\": reviewText,\n",
        "                  \"reviewRating\": reviewRating,\n",
        "                  \"bizType\": \"urgentCare\",\n",
        "                  \"dataSource\": \"Google\"}\n",
        "        reviewScaleDF = reviewScaleDF.append(new_row, ignore_index=True)\n",
        "  counter += 1\n",
        "  number = (counter/len(dataFromAPIFYDF)*100)\n",
        "  print(round(number, 2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T7m4-cZk7O4"
      },
      "source": [
        "This displays the contents of the review dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kVgcaJ0_IsC"
      },
      "source": [
        "display(reviewScaleDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-NoEeK4k-7u"
      },
      "source": [
        "This converts the review dataframe to a .csv file and downloads the .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls4b3psL_OGi"
      },
      "source": [
        "from google.colab import files\n",
        "reviewScaleDF.to_csv(\"reviewScaleUrgentCareGoogle.csv\")\n",
        "files.download(\"reviewScaleUrgentCareGoogle.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiT3vuBM_Ys_"
      },
      "source": [
        "## If you want to make a dataframe where each row is one business, and the review text is stored in a column with one big review, a concatenation of ALL that reviews for that business..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdRp5k9Yh3qJ"
      },
      "source": [
        "This code allows you to view the dataframe by each business with the reviews combined into one attribute. First, a result data fram is created with the attributes businessTitle, googleURL, CIDnum, phoneNum, address, city, state, businessRating, bigText, numberOfReviews, bizType, and dataSource. Conditions can then be used to filter out the businesses that are not desired. Here, we set the conditions to limit the APIFYDF dataframe results to only \"Urgent care centers\" and businesses in 'OH'. The code then pulls the same attributes used in the result dataframe from the businesses in the APIFYDF dataframe and stores the results. \n",
        "For each unique business, the reviews are populated. If there is more than one review, the new review is added onto the current review and separated with a new line, 10 tildes, and another new line. This continues until all of the reviews from that business have been added. The stored attributes are then added to a new row, and the new row is put into the result dataframe. This continues for all of the businesses in the APIFYDF dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeVprSTP_r21"
      },
      "source": [
        "#INSTITUTION SCALE\n",
        "\n",
        "\n",
        "institutionalScaleDF = pd.DataFrame(columns=[\"businessTitle\", \"googleURL\", \"CIDnum\", \"phoneNum\", \"address\", \"city\", \"state\", \"businessRating\", \"bigText\", \"numberOfReviews\", \"bizType\", \"dataSource\"])\n",
        "counter = 0\n",
        "for business in range(len(dataFromAPIFYDF)):\n",
        "  #Our search will have given us businesses that are not urgent care centers, and since Cincinnati is very close to Kentucky, we may get some results from KY\n",
        "  #This next step filters results to only include businesses that are in Ohio and that have the categoryName 'Urgent care center'\n",
        "  if dataFromAPIFYDF.loc[business, 'categoryName'] == 'Urgent care center' and 'OH' in dataFromAPIFYDF.loc[business, 'address']:\n",
        "\n",
        "    businessTitle = dataFromAPIFYDF.loc[business, 'title']\n",
        "    directURL = dataFromAPIFYDF.loc[business, 'placeId']\n",
        "    placeIDtoSave = dataFromAPIFYDF.loc[business, 'placeId']\n",
        "\n",
        "    theGoldenCID = placeIDtoSave.split('cid=')[1]\n",
        "\n",
        "    phoneNum = dataFromAPIFYDF.loc[business, 'phone']\n",
        "    address = dataFromAPIFYDF.loc[business, 'address']\n",
        "    city = address.split(',')[1].strip()\n",
        "    state = address.split(',')[2].strip()[0:2]\n",
        "\n",
        "    overallRating = dataFromAPIFYDF.loc[business, 'totalScore']\n",
        "\n",
        "    bigText = \"\"\n",
        "    numOfReviews = 0\n",
        "    #For each review...\n",
        "    for review in range(len(dataFromAPIFYDF.loc[business, 'reviews'])):\n",
        "      #If the review has text, meaning it's not just a rating based on number of stars...\n",
        "      if dataFromAPIFYDF.loc[business, 'reviews'][review]['text']:\n",
        "        reviewText = dataFromAPIFYDF.loc[business, 'reviews'][review]['text']\n",
        "        #...append the review to this string called big text, and separate individual reviews with a newline, 10 tildes, and another newline.\n",
        "        bigText = bigText + reviewText\n",
        "        bigText = bigText + \"\\n\" + \"~~~~~~~~~~\" + \"\\n\"\n",
        "        numOfReviews += 1\n",
        "        \n",
        "\n",
        "\n",
        "    #Make sure the business had a least ONE review with text in it...\n",
        "    if bigText:\n",
        "      new_row = {'businessTitle': businessTitle,\n",
        "                'googleURL': directURL,\n",
        "                'CIDnum': theGoldenCID,\n",
        "                'phoneNum': phoneNum,\n",
        "                'address': address,\n",
        "                'city': city,\n",
        "                'state': state,\n",
        "                'businessRating': overallRating,\n",
        "                \"bigText\": bigText,\n",
        "                \"numberOfReviews\": numOfReviews,\n",
        "                \"bizType\": \"urgentCare\",\n",
        "                \"dataSource\": \"Google\"}\n",
        "      institutionalScaleDF = institutionalScaleDF.append(new_row, ignore_index=True)\n",
        "  counter += 1\n",
        "  number = (counter/len(dataFromAPIFYDF)*100)\n",
        "  print(round(number, 2))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB9p2aJgkrzg"
      },
      "source": [
        "This displays the contents of the result dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybywe7Ds_wP2"
      },
      "source": [
        "display(institutionalScaleDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmlFMVDhkwiD"
      },
      "source": [
        "This converts the result dataframe to a .csv file and downloads the .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5_8NHQT_zig"
      },
      "source": [
        "from google.colab import files\n",
        "institutionalScaleDF.to_csv(\"institutionalScaleUrgentCareGoogle.csv\")\n",
        "files.download(\"institutionalScaleUrgentCareGoogle.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}